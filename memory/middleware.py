from datetime import datetime
from typing import Awaitable, Callable, List, cast
from langchain.agents.middleware.types import (
    AgentMiddleware,
    ModelCallResult,
    ModelRequest,
    ModelResponse,
    OmitFromInput,
)
from langchain.messages import SystemMessage
from .memos_client import MemosClient
from langchain_core.messages.base import BaseMessage
from langchain.tools import tool

SEARCH_MEMO_TOOL_DESCRIPTION = """
This tool is your access to the User's Long-Term Memory (facts, preferences, past projects).
Use this to retrieve context that is NOT in the current conversation window.
## Args:
- query: The text content to be queried in the memory
- memory_limit: The number of factual memories to be returned, with a default value of 10
- includePreference: Whether to enable the recall of preferred memories, with a default value of True
- preference_limit: The number of preferred memories to be returned, with a default value of 6

## Effective Query Strategy
- **DO NOT** use generic queries like "user history" or "preferences".
- **DO** extract specific entities or topics. 
  - *Bad*: "What did the user say?"
  - *Good*: "spicy food tolerance", "Python project config", "trip to Guangzhou"
- **Multiple Queries**: If the user asks complex questions, you can call this tool multiple times with different focus topics.

## When to Skip
- Do not search for simple greetings ("Hello").
- Do not search for general knowledge ("What is Python?").
- Do not search if the info was just mentioned 2 turns ago (it's in short-term context).
"""

MEMO_SYSTEM_PROMPT = """## ğŸ§  Long-term Memory Architecture

You are paired with a **MemOS Memory System** that stores the user's personal context.
This differentiates you from a generic AI. You "know" the user.

### 1. The "Recall First" Rule
Before answering ANY question involving choices, recommendations, or personal context, you MUST:

1.  **Analyze & Decompose**: Does this request touch on multiple aspects of the user's life? 
    -   Example: For coding questions like "Help me write simple a neural network FFN", you can search like ("User coding Language and Style", "User recent Projects" etc.).
    -   Example: For "Plan a trip", launch `search_memos("user travel preference")`,`search_memos("Where the user has traveled")`, `search_memos("user travel budget")`, and `search_memos("user dietary restriction")` simultaneously.
2.  **Parallel Search (CRITICAL)**: **DO NOT** limit yourself to a single search query.
    -   You MUST call `search_memos` **multiple times in parallel** if the context is multi-dimensional.
    -  **DO NOT** search for:
        -  simple greetings ("Hello"), or general knowledge ("What is Python?"). 
        -  if the info was just mentioned 8 turns ago (it's in short-term context).
        -   the same information multiple times, or information that is not relevant to the current conversation.
        -   the information that is too broad or general in nature.
3.  **Synthesize**: Combine all retrieved memory fragments with new info to generate a personalized response.
4.  **Evaluate**: Evaluate the relevance of the memory to the current task.


### 2. Natural Integration (Crucial)
When you call `search_memos` in your response, **DO NOT** say: "I need to check my memory..." or "let's search my memory...".
When you find a memory, **DO NOT** say: "According to the database..." or "I found a record...".
**DO** speak naturally, like an old friend remembering a fact.
- *Bad*: "I need find my memory to ensure your food preferences."
- *Good*: "Let me think for a moment about what you like to eat."
- *Bad*: "Found memory: User likes spicy food. I recommend hotpot."
- *Good*: "Since I know you enjoy spicy food, how about trying that hotpot place?"

### 3. Handling "No Memory"
If `search_memos` returns nothing:
- Proceed normally.
- **DO NOT** apologize for not knowing.
- **DO NOT** hallucinate a memory.

### 4. Conflict Resolution
- **Present > Past**: If the user's current statement contradicts a memory (e.g., Memory: "Hates cilantro", User: "I love cilantro now"), **TRUST THE USER'S CURRENT STATEMENT**.
- The **MemOS Memory System** will automatically update the memory later. You just follow the current instruction.
"""


class MemOSMiddleware(AgentMiddleware):
    memo_client = MemosClient()

    def __init__(self):
        super().__init__()
        self.system_prompt = MEMO_SYSTEM_PROMPT
        @tool(description=SEARCH_MEMO_TOOL_DESCRIPTION)
        def search_memos(
            query: str,
            memory_limit: int = 10,
            include_preference: bool = True,
            preference_limit: int = 6,
            runtime=None
        ) -> str:
            """
            Search user's long-term memory.
            Returns a clean, summarized text for the AI to read.
            """
            # 1. è·å– ID (å‡è®¾ä½ å·²ç»è§£å†³äº† runtime æ³¨å…¥é—®é¢˜ï¼Œæˆ–è€…ä½¿ç”¨é—­åŒ…æ•è·äº† user_id)
            user_id = getattr(runtime.context, "user_id", "default_user") if runtime else "default_user"
            conversation_id = getattr(runtime.context, "thread_id", None) if runtime else None
            
            # 2. è°ƒç”¨ API
            try:
                raw_result = self.memo_client.search_memory(
                    user_id,
                    query,
                    conversation_id,
                    memory_limit,
                    include_preference,
                    preference_limit,
                )
            except Exception as e:
                return f"Error retrieving memory: {str(e)}"

            # 3. æ•°æ®æ¸…æ´—ä¸æ ¼å¼åŒ– (Core Logic)
            if not raw_result:
                return "No relevant memories found."

            memories = raw_result.get("memory_detail_list", [])
            preferences = raw_result.get("preference_detail_list", [])
            
            output_lines = []

            # --- A. å¤„ç†åå¥½ (Preferences) ---
            if include_preference and preferences:
                output_lines.append("### â¤ï¸ User Preferences (High Priority)")
                
                for p in preferences:
                    # æå–æ ¸å¿ƒå­—æ®µ
                    pref_text = p.get("preference", "").strip()
                    reason = p.get("reasoning", "").strip()
                    p_type = p.get("preference_type", "implicit")
                    
                    # æ ¼å¼åŒ–
                    # [Explicit] ç”¨æˆ·å–œæ¬¢è¾£... (Reason: ...)
                    line = f"- [{p_type.capitalize()}] {pref_text}"
                    if reason:
                        line += f"\n  (Context: {reason})"
                    output_lines.append(line)
                
                output_lines.append("") # ç©ºè¡Œåˆ†éš”

            # --- B. å¤„ç†äº‹å®è®°å¿† (Fact Memories) ---
            if memories:
                output_lines.append("### ğŸ“ Relevant Facts & Context")
                
                # å»é‡é€»è¾‘ (å¯é€‰)ï¼šæœ‰æ—¶å€™å‘é‡æ£€ç´¢ä¼šè¿”å›é‡å¤å†…å®¹
                seen_values = set()
                
                for m in memories:
                    val = m.get("memory_value", "").strip()
                    
                    # 1. è¿‡æ»¤æ‰æ— æ„ä¹‰çš„ç³»ç»Ÿæ—¥å¿—æˆ– Raw Chat
                    # ä½ çš„æ•°æ®é‡Œæœ‰ä¸€äº› "user: [time]: ...", è¿™ç§æœ€å¥½æ¸…æ´—ä¸€ä¸‹æˆ–è€…ç›´æ¥å±•ç¤º
                    # å¦‚æœ memory_key æ˜¯ "assistant:" æˆ– "user:" å¼€å¤´ï¼Œä¸”å†…å®¹å¾ˆçŸ­ï¼Œå¯èƒ½ä»·å€¼ä¸é«˜
                    if len(val) < 5 or val in seen_values:
                        continue
                    
                    seen_values.add(val)
                    
                    # 2. æå–å…ƒæ•°æ®
                    m_type = m.get("memory_type", "General")
                    confidence = m.get("confidence", 0)
                    key = m.get("memory_key", "")
                    
                    # 3. æ™ºèƒ½æ ¼å¼åŒ–
                    # å¦‚æœ Key å¾ˆæœ‰æ„ä¹‰ï¼ˆä¸æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„ user:xxxï¼‰ï¼Œå°±å±•ç¤º Key
                    prefix = f"[{key}] " if key and not key.startswith(("user:", "assistant:")) else ""
                    
                    # 4. æ—¶é—´å¤„ç† (å¦‚æœæœ‰ create_time)
                    # create_time æ˜¯æ¯«ç§’çº§æ—¶é—´æˆ³ -> è½¬å¯è¯»æ—¶é—´
                    ts = m.get("create_time")
                    time_str = ""
                    if ts:
                        try:
                            dt = datetime.fromtimestamp(ts / 1000)
                            time_str = f" ({dt.strftime('%Y-%m-%d')})"
                        except:
                            pass

                    output_lines.append(f"- {prefix}{val}{time_str}")

            # --- C. æœ€ç»ˆç»„è£… ---
            if not output_lines:
                return "No relevant memories found."
            
            # æ·»åŠ å¤´éƒ¨æç¤ºï¼Œå¼ºåŒ– LLM çš„æ³¨æ„åŠ›
            final_output = "\n".join(output_lines)
            return f"[MemOS] retrieved the following context from long-term memory:\n\n{final_output}"

        self.tools = [search_memos]

    def before_agent(self, state, runtime):
        user_id = getattr(runtime.context, "user_id", "default_user")
        conversation_id = getattr(runtime.context, "thread_id", "default_conversation")
        self.memo_client.add_messages(
            user_id, conversation_id, self.messages_to_dicts(state["messages"][-1:])
        )

    def after_model(self, state, runtime):
        user_id = getattr(runtime.context, "user_id", "default_user")
        conversation_id = getattr(runtime.context, "thread_id", "default_conversation")
        self.memo_client.add_messages(
            user_id, conversation_id, self.messages_to_dicts(state["messages"][-1:])
        )

    def messages_to_dicts(self, messages: List[BaseMessage]):
        """å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸º dict åˆ—è¡¨"""
        convert_dict = {
            "human": "user",
            "ai": "assistant",
        }
        return [
            {
                "role": convert_dict.get(msg.type, "user"),
                "content": msg.content,
                "chat_time": msg.response_metadata.get(
                    "timestamp", datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                ),
            }
            for msg in messages
        ]

    def add_timestamp_to_state(self, state):
        if not state.get("messages"):
            return None
        messages_without_timestamp = self.rfind_messages_without_timestamp(
            state["messages"]
        )
        return {"messages": self.add_timestamp_to_messages(messages_without_timestamp)}

    def add_timestamp_to_messages(self, messages):
        for message in messages:
            if hasattr(message, "response_metadata"):
                # åˆ›å»ºæ–°çš„ metadata å‰¯æœ¬å¹¶æ·»åŠ æ—¶é—´æˆ³
                if "timestamp" not in message.response_metadata:
                    updated_metadata = dict(message.response_metadata or {})
                    updated_metadata["timestamp"] = datetime.now().strftime(
                        "%Y-%m-%d %H:%M:%S"
                    )
                    message.response_metadata = updated_metadata

        return messages

    def rfind_messages_without_timestamp(self, messages):
        messages_without_timestamp = []
        for message in reversed(messages):
            if "timestamp" not in message.response_metadata:
                messages_without_timestamp.append(message)
            else:
                break
        messages_without_timestamp = list(reversed(messages_without_timestamp))
        self.new_messages.extend(messages_without_timestamp)
        return messages_without_timestamp

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelCallResult:
        """Update the system message to include the todo system prompt."""
        if request.system_message is not None:
            new_system_content = [
                *request.system_message.content_blocks,
                {"type": "text", "text": f"\n\n{self.system_prompt}"},
            ]
        else:
            new_system_content = [{"type": "text", "text": self.system_prompt}]
        new_system_message = SystemMessage(
            content=cast("list[str | dict[str, str]]", new_system_content)
        )
        return handler(request.override(system_message=new_system_message))

    async def awrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], Awaitable[ModelResponse]],
    ) -> ModelCallResult:
        """Update the system message to include the todo system prompt (async version)."""
        if request.system_message is not None:
            new_system_content = [
                *request.system_message.content_blocks,
                {"type": "text", "text": f"\n\n{self.system_prompt}"},
            ]
        else:
            new_system_content = [{"type": "text", "text": self.system_prompt}]
        new_system_message = SystemMessage(
            content=cast("list[str | dict[str, str]]", new_system_content)
        )
        return await handler(request.override(system_message=new_system_message))
